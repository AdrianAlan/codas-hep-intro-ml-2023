{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8251bc6a",
   "metadata": {},
   "source": [
    "# CoDaS-HEP 2023: Convolutional Neural Networks and Autoencoders\n",
    "\n",
    "Meterial: Working with images, Convolutional Neural Networks (CNNs), variational autoencoders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee034aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa6a8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If your are running on Google Collab, execute these lines as well\n",
    "\n",
    "from google.colab import drive\n",
    "\n",
    "# Mount GDrive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Maker directory\n",
    "%mkdir /content/drive/MyDrive/Github/\n",
    "\n",
    "# Change directory\n",
    "%cd /content/drive/MyDrive/Github/\n",
    "\n",
    "# Clone the repository\n",
    "!git clone https://github.com/AdrianAlan/codas-hep-intro-ml-2023.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12013223",
   "metadata": {},
   "source": [
    "## Working with Images and Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddbf878",
   "metadata": {},
   "source": [
    "In this part you will learn how to build and train a Convolutional Neural Network (CNN). For this exercise, we will use the dataset containing *jet images* and corresponding labels.\n",
    "\n",
    "Traditional approaches to jet tagging rely on features, such as jet substructure, designed by experts that detect characteristic energy deposit patterns. In recent years, many studies applied computer vision for event reconstruction at particle colliders. This was obtained by projecting the lower level detector measurements of the emanating particles onto a cylindrical detector and then unwrapping the inner surface of the calorimeter on a rectangle. Such information was further interpreted as an image with calorimeter cells as pixels, where pixel intensity maps the energy deposit of the cell, i.e. **jet images**. The different appearance of these jets can be used as a handle to discriminate between them, i.e. **jet tagging**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f8d522",
   "metadata": {},
   "source": [
    "### Convolution Operation\n",
    "Two-dimensional convolutional layer for image height $H$, width $W$, number of input channels $C$, number of output kernels (filters) $N$, and kernel height $J$ and width $K$ is given by:\n",
    "\n",
    "\\begin{align}\n",
    "\\label{convLayer}\n",
    "\\boldsymbol{Y}[v,u,n] &= \\boldsymbol{\\beta}[n] + \\sum_{c=1}^{C} \\sum_{j=1}^{J} \\sum_{k=1}^{K} \\boldsymbol{X}[v+j,u+k,c]\\, \\boldsymbol{W}[j,k,c,n]\\,,\n",
    "\\end{align}\n",
    "\n",
    "where $Y$ is the output tensor of size $V \\times U \\times N$, $W$ is the weight tensor of size $J \\times K \\times C \\times N$ and $\\beta$ is the bias vector of length $N$ .\n",
    "\n",
    "The example below uses a sharpening filter with $C=1$, $J=K=3$:\n",
    "\n",
    "<img src=\"assets/convolution.gif\"/>[Credit](https://towardsdatascience.com/types-of-convolution-kernels-simplified-f040cb307c37)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2b0019",
   "metadata": {},
   "source": [
    "Let's write our own convolution operation and see how does it perform..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5b8b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolution(img, kernel, stride=None, padding=None):\n",
    "    \n",
    "    H = img.shape[0] # height of the image\n",
    "    W = img.shape[1] # width of the image\n",
    "    \n",
    "    # Let's assume S = J = K\n",
    "    S = kernel.shape[0]\n",
    "    filt_h = kernel.shape[1]\n",
    "    \n",
    "    # In our case we only have one filer, so n=1. We will drop the 3rd dim for simplicity\n",
    "    img_out = np.zeros((H+1,W+1))\n",
    "\n",
    "    # Nested loops over V and U\n",
    "    for v in range(S//2, H-S//2):\n",
    "        for u in range(S//2, W-S//2):\n",
    "            img_out[v, u] = np.sum(img[v-S//2:v+S//2+1,u-S//2:u+S//2+1]*kernel)\n",
    "    return img_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9c092f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and show an image\n",
    "\n",
    "img = mpimg.imread('assets/experiment.jpg')\n",
    "\n",
    "plt.imshow(img[:, :, 0]);\n",
    "\n",
    "# The CMS detecor:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422bd078",
   "metadata": {},
   "source": [
    "A lot of filters were hand-crafted a long time ago. Apps on your phone are often still using these man-made kernes. We are going to to use an classic edge detection filter: a Sobel filter. You can read more about it [here](https://en.wikipedia.org/wiki/Sobel_operator)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89105326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our filter\n",
    "\n",
    "sobel_filter = np.array(\n",
    "    [\n",
    "        [ 1,  2,  1],\n",
    "        [ 0,  0,  0],\n",
    "        [-1, -2, -1]\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5907fa66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform our convolution and show the result\n",
    "\n",
    "img_edges = convolution(img[:, :, 0], sobel_filter)\n",
    "\n",
    "plt.imshow(img_edges, cmap='bwr');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae43408",
   "metadata": {},
   "source": [
    "The edges should be red or blue colored (depending on the direction of change), gray areas mean no edge. We have used a vertical variant of the sobel filter. We can see the edges quite clear. Let's try to see if there is any difference when we apply a horizontal version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ac6f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform our convolution and show the result\n",
    "\n",
    "img_edges = convolution(img[:, :, 0], sobel_filter.T)\n",
    "\n",
    "plt.imshow(img_edges, cmap='bwr');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213e1497",
   "metadata": {},
   "source": [
    "Let's do a check if our input image shape matches the shape of the output one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff58f554",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert img[:, :, 0].shape == img_edges.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fe1e89",
   "metadata": {},
   "source": [
    "We should expect that because we didn't apply any padding. There are two important parameters that we did not implement, i.e. stride and padding. **Stride** contols by how many pixels do we move in every loop. **Padding** expands the input by adding border to the image.\n",
    "\n",
    "**Your task**: implement padding (*and optionally stride*). You could use `np.pad` method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37dfa720",
   "metadata": {},
   "source": [
    "### Pooling Operation\n",
    "\n",
    "Pooling layers are used to reduce the dimensions of the feature maps, which reduces number of learnable parameters and speeds forward and backward pass. The operation slides a filter over the feature map which either outputs and average or max."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9572e5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pooling(img, size):\n",
    "    # Mind that I used size as stride as in preactis they are often equal.\n",
    "\n",
    "    H = img.shape[0] # height of the image\n",
    "    W = img.shape[1] # width of the image\n",
    "\n",
    "    img_out = np.zeros((H//size, W//size))\n",
    "\n",
    "    # Nested loops over V and U\n",
    "    for v in range(0, H//size):\n",
    "        for u in range(0, W//size):\n",
    "            img_out[v, u] = np.max(img[v*size:v*size+size, u*size:u*size+size])\n",
    "    return img_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ea99f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform our pooling and show the result\n",
    "\n",
    "img_pool = pooling(img[:, :, 0], 4)\n",
    "\n",
    "plt.imshow(img_pool);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153dd484",
   "metadata": {},
   "source": [
    "## Jet Tagging: Introduction\n",
    "\n",
    "The majority of particles produced in LHC events are unstable and immediately decay to lighter particles. The new particles can decay themselves to others in a so-called decay chain. Such a process terminates when the decay products are stable particles, e.g., charged pions. This collimated shower of particles with adjacent trajectories is called a *jet*. Jets are central to many physics studies at the LHC experiments. In particular, a successful physics program requires aggregating particles into jets (jet clustering), an accurate determination of the jet momentum (momentum measurement) and the identification of which particle kind started the shower (**jet tagging**).\n",
    "\n",
    "In this excercise you will learn how to train and evaluate a classifier that will predict which particle produced a given jet, a *jet tagger*. We will classify jets to five classes: produced by decay of top (*t*), Z (*z*) and W (*w*) boson, gluon (*g*) and quark (*q*)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e91da3",
   "metadata": {},
   "source": [
    "### Preparation of the training and validation samples\n",
    "\n",
    "If you have already downloaded the datasets you can skip the cell below and. In order to import the dataset: clone the dataset repository (to import the data in Colab), load the `h5` files in the data/ repository, extract the data we need: a target and jetImage.\n",
    "\n",
    "To type shell commands, we start the command line with !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5751d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! curl https://cernbox.cern.ch/s/6Ec5pGFEpFWeH6S/download -o ../Data-MLtutorial.tar.gz\n",
    "# ! tar -xvzf ../Data-MLtutorial.tar.gz -C ../\n",
    "# ! ls ../Data-MLtutorial/JetDataset/\n",
    "# ! rm ../Data-MLtutorial.tar.gz "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f394a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "\n",
    "target = np.array([])\n",
    "inputs = np.array([])\n",
    "\n",
    "datafiles = ['../Data-MLtutorial/JetDataset/jetImage_7_100p_30000_40000.h5',\n",
    "             '../Data-MLtutorial/JetDataset/jetImage_7_100p_50000_60000.h5',\n",
    "             '../Data-MLtutorial/JetDataset/jetImage_7_100p_10000_20000.h5',\n",
    "             '../Data-MLtutorial/JetDataset/jetImage_7_100p_0_10000.h5']\n",
    "\n",
    "for file_ in datafiles:\n",
    "    with h5py.File(file_, 'r') as f:\n",
    "        print(\"Appending {}\".format(file_))\n",
    "        tmp_jet_img = np.array(f.get(\"jetImage\"))\n",
    "        tmp_target = np.array(f.get('jets')[0:,-6:-1])\n",
    "        inputs = np.concatenate([inputs, tmp_jet_img], axis=0) if inputs.size else tmp_jet_img\n",
    "        target = np.concatenate([target, tmp_target], axis=0) if target.size else tmp_target\n",
    "          \n",
    "print(target.shape, inputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffb0801",
   "metadata": {},
   "source": [
    "Each `target` row is a one-hot encoded class, where the labels correspond to this order: `['gluon', 'quark', 'W', 'Z', 'top']`. For example $[1, 0, 0, 0, 0]$ means gluon and $[0, 0, 0, 0, 1]$ top jet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d714564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how does the top boson looks like\n",
    "\n",
    "average_w = np.mean(inputs[:1000][target[:1000, -1] == 1], axis=0)\n",
    "plt.title('top')\n",
    "plt.imshow(average_w);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead5f13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And here is an average of gluon?\n",
    "\n",
    "average_gluon = np.mean(inputs[:1000][target[:1000, 0] == 1], axis=0)\n",
    "plt.title('gluon')\n",
    "plt.imshow(average_gluon);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbbe60c",
   "metadata": {},
   "source": [
    "Let's prepare the dataset for training..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d609a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train, Validation, Test split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(inputs, target, test_size=0.4)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "print(y_train.shape)\n",
    "print(y_val.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4983e5",
   "metadata": {},
   "source": [
    "In `keras`, images are represented as $n \\times m \\times k$ tensors, where $n \\times m$ are the pixel dimenions and $k$ is the number of channels (e.g., 1 in a black\\&while image, 3 for an RGB image). In our case, k=1. To comply to this, we add the channel index by reshaping the image dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b407211",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.expand_dims(X_train, axis=-1)\n",
    "X_val = np.expand_dims(X_val, axis=-1)\n",
    "X_test = np.expand_dims(X_test, axis=-1)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb464ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras imports\n",
    "\n",
    "from tensorflow.keras.models import Model, model_from_json\n",
    "from tensorflow.keras.layers import Dense, Input, Conv2D, Dropout, Flatten, BatchNormalization\n",
    "from tensorflow.keras.layers import MaxPooling2D, Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bf2829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your CNN architecture using Keras functional API\n",
    "\n",
    "input_ = Input(shape=(100, 100, 1))\n",
    "x = Conv2D(5, kernel_size=(5, 5), data_format=\"channels_last\", strides=(1, 1), padding=\"same\")(input_)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = MaxPooling2D(pool_size = (5, 5))(x)\n",
    "x = Dropout(0.25)(x)\n",
    "x = Conv2D(3, kernel_size=(3, 3), data_format=\"channels_last\", strides=(1, 1), padding=\"same\")(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = MaxPooling2D( pool_size = (3,3))(x)\n",
    "x = Dropout(0.25)(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(5, activation='relu')(x)\n",
    "output = Dense(5, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=input_, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75363672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# Print model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21e29ee",
   "metadata": {},
   "source": [
    "We now train the model. This takes really long time and processing power on common CPUs. If you are running locally set `EPOCHS=1` just to continue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843ba7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20\n",
    "\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=32,\n",
    "    verbose=2,\n",
    "    validation_data=(X_val, y_val)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b50b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "\n",
    "with open(\"../Data-MLtutorial/jetTagger_CNN.json\", \"w\") as json_file:\n",
    "    json_file.write(model.to_json())\n",
    "model.save_weights(\"../Data-MLtutorial/jetTagger_CNN.h5\")\n",
    "\n",
    "with open('../Data-MLtutorial/history.h5', 'wb') as f:\n",
    "    pickle.dump(history.history, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef18df38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "\n",
    "history = history.history\n",
    "plt.plot(history['loss'])\n",
    "plt.plot(history['val_loss'])\n",
    "plt.yscale('log')\n",
    "plt.title('Training History')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Training', 'Validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a363bc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curve and corresponding AUC\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "plt.figure()\n",
    "for i, label in enumerate(['gluon', 'quark', 'W', 'Z', 'top']):\n",
    "    fpr, tpr, _ = roc_curve(y_test[:, i], y_pred[:, i])\n",
    "    plt.plot(tpr, fpr, label='{0} tagger, AUC = {1:.1f}'.format(label, auc(fpr, tpr)*100.))\n",
    "\n",
    "plt.semilogy()\n",
    "plt.xlabel(\"sig. efficiency\")\n",
    "plt.ylabel(\"bkg. mistag rate\")\n",
    "plt.ylim(0.000001, 1)\n",
    "plt.grid(True)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d321e5ad",
   "metadata": {},
   "source": [
    "## Variational Autoencoders\n",
    "\n",
    "Variational Autoencoders (VAEs) learn a latent variable model the input data. Instead of having a deterministic mapping like in vanilla autoencoder, VAE learns a probability distribution over the latent variables. When sampling from this distribution, we can generate new, artificial data samples. Hence, we say that a VAE is a generative model. Let's write a VAE model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabbd480",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "from tensorflow.keras.layers import Input, Flatten, Dense, Lambda, Reshape, Conv2D, Conv2DTranspose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269a54c7",
   "metadata": {},
   "source": [
    "We need to define a function that takes in the mean and log variance parameters and return a random sample from the resulting distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac5b83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling(args):\n",
    "    mean, logvar = args\n",
    "    eps = K.random_normal([latent_dim])\n",
    "    rnd_sam = mean + K.exp(logvar/2) * eps\n",
    "    return rnd_sam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceff57b6",
   "metadata": {},
   "source": [
    "Let's write the encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a58965a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder\n",
    "enc_input = Input(shape=(28, 28, 1), name='encoder_input')\n",
    "x = Conv2D(32, 3, padding='same', strides=2, activation='relu')(enc_input)\n",
    "x = Conv2D(64, 3, padding='same', strides=2, activation='relu')(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(16)(x)\n",
    "\n",
    "# Latent space\n",
    "latent_dim = 2\n",
    "z_mean = Dense(latent_dim, name='Z-mean')(x)\n",
    "z_logvar = Dense(latent_dim, name='Z-logvariance')(x)\n",
    "\n",
    "z = Lambda(sampling, output_shape=latent_dim, name='latent-space')([z_mean, z_logvar])\n",
    "\n",
    "encoder = Model(enc_input, z, name='encoder')\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f34c163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder\n",
    "dec_input = Input(shape=(latent_dim,), name='decoder_input')\n",
    "\n",
    "y = Dense(7 * 7 * 64)(dec_input)\n",
    "y = Reshape(target_shape=(7, 7, 64))(y)\n",
    "y = Conv2DTranspose(64, 3, padding='same', strides=2, activation='relu')(y)\n",
    "y = Conv2DTranspose(32, 3, padding='same', strides=2, activation='relu')(y)\n",
    "y = Conv2DTranspose(1, 3, padding='same', activation='sigmoid')(y)\n",
    "\n",
    "decoder = Model(dec_input, y, name='decoder')\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db852d1",
   "metadata": {},
   "source": [
    "We trained the model with a loss consisting of two parts. A reconstruction part forces the decoded samples to match the initial inputs. The KL-divergence between the learned latent distribution and the prior distribution (Gaussian), acting as a regularization term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7271934d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_func(z_mean, z_logvar):\n",
    "\n",
    "    def vae_reconstruction_loss(y_true, y_pred):\n",
    "        return K.mean(K.sum(K.square(y_true - y_pred), axis=[1, 2]))\n",
    "\n",
    "    def vae_kl_loss(z_mean, z_logvar):\n",
    "        return -0.5 * K.mean(1.0 + z_logvar - K.square(z_mean) - K.exp(z_logvar), axis=1)\n",
    "\n",
    "    def vae_loss(y_true, y_predict, beta=1):\n",
    "        reconstruction_loss = vae_reconstruction_loss(y_true, y_predict)\n",
    "        kl_loss = vae_kl_loss(y_true, y_predict)\n",
    "        return reconstruction_loss + beta * kl_loss\n",
    "\n",
    "    return vae_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6135b490",
   "metadata": {},
   "source": [
    "Note: it is very common to introduce a $\\beta$ parameter that regulates the strength of the second term. It is very common that KL-divergence falls to the prior completetely, i.e. `kl_divergence = 0`. This prevents the model to learn anything useful. To combat that we set $\\beta < 1$ and commonly increase it after initial phases of the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971f06c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connecting encoder and decoder\n",
    "\n",
    "enc_output = encoder(enc_input)\n",
    "dec_output = decoder(enc_output)\n",
    "\n",
    "vae = Model(enc_input, dec_output, name='VAE')\n",
    "vae.summary()\n",
    "\n",
    "opt = tensorflow.keras.optimizers.Adam()\n",
    "vae.compile(optimizer=opt, loss=loss_func(z_mean, z_logvar))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5625a77",
   "metadata": {},
   "source": [
    "For this exercise we will use the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac410d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load samples of the digits\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = tensorflow.keras.datasets.mnist.load_data()\n",
    "\n",
    "X_train = X_train.astype('float32') / 255\n",
    "X_test = X_test.astype('float32') / 255\n",
    "\n",
    "X_train = X_train.reshape((-1,28,28,1))\n",
    "X_test = X_test.reshape((-1,28,28,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b44add6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train VAE\n",
    "\n",
    "history = vae.fit(\n",
    "    X_train,\n",
    "    X_train,\n",
    "    epochs=30,\n",
    "    batch_size=128,\n",
    "    validation_data=(X_test, X_test)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e71c3dd",
   "metadata": {},
   "source": [
    "### VAEs: Generating New Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8963bfd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot a few samples\n",
    "\n",
    "figure1, axis1 = plt.subplots(1, 10)\n",
    "figure2, axis2 = plt.subplots(1, 10)\n",
    "\n",
    "for i in range(10):\n",
    "    axis1[i].get_xaxis().set_visible(False)\n",
    "    axis1[i].get_yaxis().set_visible(False)\n",
    "    axis1[i].imshow(X_test[i])\n",
    "\n",
    "    axis2[i].get_xaxis().set_visible(False)\n",
    "    axis2[i].get_yaxis().set_visible(False)\n",
    "    axis2[i].imshow(vae.predict(X_test[i].reshape(1, 28, 28, 1)).reshape(28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de3c77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try to generate few new samples using out decoder\n",
    "\n",
    "figure1, axis1 = plt.subplots(1, 10)\n",
    "\n",
    "code_for_A = encoder(X_test[0].reshape(1, 28, 28, 1))[0]\n",
    "code_for_B = encoder(X_test[2].reshape(1, 28, 28, 1))[0]\n",
    "\n",
    "code_for_A = code_for_A.numpy()\n",
    "code_for_B = code_for_B.numpy()\n",
    "\n",
    "interpolation = np.linspace(code_for_A, code_for_B, 10)\n",
    "\n",
    "for i, code in enumerate(interpolation):\n",
    "    img = decoder(code.reshape(1, 2))\n",
    "    axis1[i].get_xaxis().set_visible(False)\n",
    "    axis1[i].get_yaxis().set_visible(False)\n",
    "    axis1[i].imshow(img.numpy().reshape(28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494927c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
